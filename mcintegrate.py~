import numpy as np

class MCIntegration():


    def __init__(self, func, lower, upper, dim):
        self.f = func
        self.a = lower
        self.b = upper
        self.n = dim

    def integrate(self, method = 'metropolis_hastings', N = 1000, diagnostics = 'on', proposal = None):
        if proposal is None:
            if n = 1:
                self.proposal = np.random.uniform(a, b, N)
            else:

        method = method.strip()
        points = self.method(N)
        mean, errorterm = f_mean(points)
        integral = 1/N*mean*self.measure()
        error = np.sqrt(errorterm/self.n - integral**2)/np.sqrt(self.n)
        if diagnostics = 'on':
            return integral, self.run_diagnostics(integral)
        return integral, error

    def f_mean(points):
        values = [self.f(x.flatten) for x in points]
        return np.sum(values), np.sum([f**2 for f in values])


    def measure(self):
        return np.multiply([b-a for a, b in self.a, self.b])



    def run_diagnostics(self, integral):



    def prior(x):
    #x[0] = mu, x[1]=sigma (new or current)
    #returns 1 for all valid values of sigma. Log(1) =0, so it does not affect the summation.
    #returns 0 for all invalid values of sigma (<=0). Log(0)=-infinity, and Log(negative number) is undefined.
    #It makes the new sigma infinitely unlikely.
        if(x[1] <=0):
            return 0
        return 1

    #Computes the likelihood of the data given a sigma (new or current)
    def normal_log_likelihood(x,sample):
        #x[0]=mu, x[1]=sigma (new or current)
        #data = the observation
        return np.sum(-np.log(x[1] * np.sqrt(2* np.pi) )-((sample-x[0])**2) / (2*x[1]**2))


    #Defines whether to accept or reject the new sample
    def acceptance(x, x_new):
        if x_new>x:
            return True
        else:
            accept=np.random.uniform(0,1)
            # Since we did a log likelihood, we need to exponentiate in order to compare to the random number
            # less likely x_new are less likely to be accepted
        return (accept < (np.exp(x_new-x)))

    def naive(N=1000):
        x = [np.random.normal(a, b, N) for a, b in self.a, self.b]
        return x.flatten()





    def metropolis_hastings(self, N = 1000,sample):
    # likelihood_computer(x,data): returns the likelihood that these parameters generated the data
    # transition_model(x): a function that draws a sample from a symmetric distribution and returns it
    # param_init: a starting sample
    # iterations: number of accepted to generated
    # data: the data that we wish to model
    # acceptance_rule(x,x_new): decides whether to accept or reject the new sample
        transition_model = lambda x: [x[0],np.random.normal(x[1],0.5,(1,))]
        for i in range(0, self.n):

        mu = self.sample.mean()
        x = [mu, sigma]
        accepted = []
        rejected = []
        for i in range(iterations):
            x_new =  transition_model(x)
            x_lik = self.normal_log_likelihood(x,sample)
            x_new_lik = self.normal_log_likelihood(x_new,data)
            if (self.acceptance(x_lik + np.log(prior(x)),x_new_lik+np.log(prior(x_new)))):
                x = x_new
                accepted.append(x_new)
            else:
                rejected.append(x_new)

        return np.array(accepted), np.array(rejected)

        def gibbs(self, N)
